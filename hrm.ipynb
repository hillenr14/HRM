{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df8aeb7-5ad2-4628-82cb-09d6402a5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hillenr/dev/ml/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b3560b-1a95-42a2-87a3-a33527c41a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "Compute capability: (7, 5)\n",
      "CUDA build: 12.4\n",
      "cuDNN: 90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"CUDA build:\", torch.version.cuda)\n",
    "    print(\"cuDNN:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fdf45f6-a68a-4a96-8215-474dbf5ff06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv: 100%|██████████████████████████████| 719M/719M [00:07<00:00, 101MB/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [01:47<00:00,  9.32it/s]\n",
      "test.csv: 100%|████████████████████████████| 79.4M/79.4M [00:01<00:00, 53.3MB/s]\n",
      "100%|██████████████████████████████| 422786/422786 [00:00<00:00, 2004964.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download and build Sudoku dataset\n",
    "!python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85b8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Pretty printing utilities\n",
    "# ---------------------------\n",
    "def pretty_print_sudoku(grid: np.ndarray):\n",
    "    if isinstance(grid, torch.Tensor):\n",
    "        grid = grid.detach().cpu().numpy()\n",
    "    grid = grid.astype(int)\n",
    "    for i in range(9):\n",
    "        if i % 3 == 0 and i != 0:\n",
    "            print(\"- - - - - - - - - - -\")\n",
    "        row = []\n",
    "        for j in range(9):\n",
    "            if j % 3 == 0 and j != 0:\n",
    "                row.append(\"|\")\n",
    "            row.append(str(grid[i, j]-1))\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "236f5eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sudoku-extreme-1k-aug-1000/train/\n",
      "all__puzzle_identifiers.npy - ndim: 1, shape: (1001000,)\n",
      "all__group_indices.npy - ndim: 1, shape: (1001,)\n",
      "all__labels.npy - ndim: 2, shape: (1001000, 81)\n",
      "1 8 5 | 2 7 3 | 4 9 6\n",
      "7 9 6 | 5 8 4 | 3 1 2\n",
      "2 3 4 | 6 9 1 | 5 8 7\n",
      "- - - - - - - - - - -\n",
      "6 2 9 | 4 5 8 | 7 3 1\n",
      "8 5 7 | 1 3 9 | 2 6 4\n",
      "4 1 3 | 7 2 6 | 8 5 9\n",
      "- - - - - - - - - - -\n",
      "9 4 2 | 3 1 5 | 6 7 8\n",
      "5 7 1 | 8 6 2 | 9 4 3\n",
      "3 6 8 | 9 4 7 | 1 2 5\n",
      "all__puzzle_indices.npy - ndim: 1, shape: (1001001,)\n",
      "all__inputs.npy - ndim: 2, shape: (1001000, 81)\n",
      "0 0 0 | 0 0 3 | 0 9 0\n",
      "0 9 6 | 0 0 4 | 0 0 0\n",
      "2 0 0 | 0 0 0 | 5 0 0\n",
      "- - - - - - - - - - -\n",
      "0 0 9 | 4 5 0 | 0 3 0\n",
      "0 5 0 | 1 3 0 | 2 0 4\n",
      "0 0 0 | 0 2 0 | 0 0 9\n",
      "- - - - - - - - - - -\n",
      "0 0 0 | 0 1 0 | 6 0 8\n",
      "5 7 0 | 0 0 2 | 0 0 0\n",
      "0 0 0 | 9 0 0 | 0 0 0\n",
      "\n",
      "data/sudoku-extreme-1k-aug-1000/test/\n",
      "all__puzzle_identifiers.npy - ndim: 1, shape: (422786,)\n",
      "all__group_indices.npy - ndim: 1, shape: (422787,)\n",
      "all__labels.npy - ndim: 2, shape: (422786, 81)\n",
      "5 9 8 | 4 6 1 | 2 7 3\n",
      "7 3 1 | 5 2 8 | 4 9 6\n",
      "2 6 4 | 3 9 7 | 5 8 1\n",
      "- - - - - - - - - - -\n",
      "8 7 9 | 6 3 2 | 1 4 5\n",
      "4 2 6 | 1 5 9 | 7 3 8\n",
      "1 5 3 | 8 7 4 | 9 6 2\n",
      "- - - - - - - - - - -\n",
      "6 1 5 | 9 4 3 | 8 2 7\n",
      "3 4 2 | 7 8 5 | 6 1 9\n",
      "9 8 7 | 2 1 6 | 3 5 4\n",
      "all__puzzle_indices.npy - ndim: 1, shape: (422787,)\n",
      "all__inputs.npy - ndim: 2, shape: (422786, 81)\n",
      "0 9 0 | 0 0 1 | 2 0 0\n",
      "0 3 0 | 0 2 8 | 4 0 6\n",
      "0 6 0 | 0 0 0 | 0 8 0\n",
      "- - - - - - - - - - -\n",
      "0 7 0 | 0 0 0 | 1 4 0\n",
      "0 2 0 | 0 5 0 | 7 0 0\n",
      "0 0 3 | 0 0 0 | 0 0 2\n",
      "- - - - - - - - - - -\n",
      "0 1 0 | 9 0 0 | 0 0 0\n",
      "0 0 0 | 7 0 5 | 0 0 0\n",
      "0 8 7 | 2 0 6 | 0 0 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for dir in [\"data/sudoku-extreme-1k-aug-1000/train/\", \"data/sudoku-extreme-1k-aug-1000/test/\"]:\n",
    "    print(dir)\n",
    "    with os.scandir(dir) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file():  # Check if the entry is a file (not a subdirectory)\n",
    "                if entry.name.endswith(\".npy\"):\n",
    "                    array = np.load(dir + entry.name, allow_pickle=True)\n",
    "                    print(f\"{entry.name} - ndim: {array.ndim}, shape: {array.shape}\")\n",
    "                    if array.ndim == 2:\n",
    "                        pretty_print_sudoku(array[0,:].reshape(9,9))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e00e3f84-db76-4dfe-9c6b-26672ed52fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]Downloading 'all_config.yaml' to 'checkpoints/sudoku/.cache/huggingface/download/G-AS8IAG4FreZDu1U09ksncp270=.c1247b366c02ae2b01f1f8c8308b60e85b5ea894.incomplete'\n",
      "\n",
      "all_config.yaml: 100%|█████████████████████████| 854/854 [00:00<00:00, 2.17MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/sudoku/all_config.yaml\n",
      "Downloading '.gitattributes' to 'checkpoints/sudoku/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.072ab407b2ed3147d7dc6acda771c244466fc4bd.incomplete'\n",
      "\n",
      ".gitattributes: 1.57kB [00:00, 1.37MB/s]\n",
      "Download complete. Moving file to checkpoints/sudoku/.gitattributes\n",
      "Fetching 3 files:  33%|█████████                  | 1/3 [00:00<00:00,  2.88it/s]Downloading 'checkpoint' to 'checkpoints/sudoku/.cache/huggingface/download/TCtVUeJteg8BwkIET3RKU6YwXzc=.0befd4a9f33a090d46df00728160ddbd69b81bcfdb8064ccfde0c867dd3836b9.incomplete'\n",
      "\n",
      "checkpoint:   0%|                                    | 0.00/109M [00:00<?, ?B/s]\u001b[A\n",
      "checkpoint:  39%|██████████▍                | 42.1M/109M [00:03<00:05, 11.8MB/s]\u001b[A\n",
      "checkpoint: 100%|████████████████████████████| 109M/109M [00:03<00:00, 27.4MB/s]\u001b[A\n",
      "Download complete. Moving file to checkpoints/sudoku/checkpoint\n",
      "Fetching 3 files: 100%|███████████████████████████| 3/3 [00:04<00:00,  1.49s/it]\n",
      "/home/hillenr/dev/ml/HRM/checkpoints/sudoku\n"
     ]
    }
   ],
   "source": [
    "# Sudoku 9x9 Extreme (1k)\n",
    "!hf download sapientinc/HRM-checkpoint-sudoku-extreme --repo-type model --local-dir checkpoints/sudoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a5b517-c190-4327-a36a-31cc82a86f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n",
      "W0827 21:28:45.358000 28352 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/hillenr/dev/ml/.venv/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py:1948: UserWarning: NVIDIA GeForce RTX 2060 with Max-Q Design does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Example: evaluate Sudoku 1k\n",
    "!python evaluate.py checkpoint=\"checkpoints/sudoku/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e65ae4-fd30-4145-bcc3-1f7a26c4e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, yaml, random\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from utils.functions import load_model_class\n",
    "# ---------------------------\n",
    "# Dataset loading\n",
    "# ---------------------------\n",
    "def load_sudoku_metadata(data_path: str) -> Dict[str, Any]:\n",
    "    test_dir = os.path.join(data_path, \"test\")\n",
    "    with open(os.path.join(test_dir, \"dataset.json\"), \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_random_sudoku_example(data_path: str, puzzle_index: Optional[int] = None) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "    \"\"\"Loads a single example from the Sudoku test set as a batch of size 1.\"\"\"\n",
    "    test_dir = os.path.join(data_path, \"test\")\n",
    "    with open(os.path.join(test_dir, \"dataset.json\"), \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Sudoku builder uses a single set named \"all\"\n",
    "    assert len(meta[\"sets\"]) == 1, \"Sudoku test set is expected to have a single subset\"\n",
    "    set_name = meta[\"sets\"][0]\n",
    "\n",
    "    inputs = np.load(os.path.join(test_dir, f\"{set_name}__inputs.npy\"))\n",
    "    labels = np.load(os.path.join(test_dir, f\"{set_name}__labels.npy\"))\n",
    "    # For Sudoku, puzzle_identifiers is all zeros\n",
    "    puzzle_identifiers = np.zeros((inputs.shape[0],), dtype=np.int32)\n",
    "\n",
    "    assert inputs.shape[1] == 81 and labels.shape[1] == 81, \"Sudoku expects 9x9 (seq_len=81)\"\n",
    "\n",
    "    n = inputs.shape[0]\n",
    "    if puzzle_index is None:\n",
    "        puzzle_index = random.randrange(n)\n",
    "\n",
    "    batch = {\n",
    "        \"inputs\": torch.from_numpy(inputs[puzzle_index:puzzle_index + 1].astype(np.int32)),\n",
    "        \"labels\": torch.from_numpy(labels[puzzle_index:puzzle_index + 1].astype(np.int32)),\n",
    "        \"puzzle_identifiers\": torch.from_numpy(puzzle_identifiers[puzzle_index:puzzle_index + 1]),\n",
    "    }\n",
    "    return batch, puzzle_index\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Checkpoint/model loading\n",
    "# ---------------------------\n",
    "def load_model_from_checkpoint(\n",
    "    *,\n",
    "    device: torch.device,\n",
    "    data_meta: Dict[str, Any],\n",
    "    checkpoint_dir: Optional[str] = None,\n",
    "    ckpt_path: Optional[str] = None,\n",
    "    hf_repo: str = \"sapientinc/HRM-checkpoint-sudoku-extreme\",\n",
    "    hf_filename: str = \"model.pt\",\n",
    "):\n",
    "    \"\"\"Builds the model and loads weights from either a local training directory or HF.\"\"\"\n",
    "    model = None\n",
    "\n",
    "    def _normalize_state_keys(state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # Handle torch.compile wrapper and loss-head wrapper prefixes\n",
    "        def strip_prefix(k: str, p: str) -> str:\n",
    "            return k[len(p):] if k.startswith(p) else k\n",
    "\n",
    "        out = {}\n",
    "        for k, v in state.items():\n",
    "            k2 = strip_prefix(k, \"_orig_mod.\")\n",
    "            k2 = strip_prefix(k2, \"model.\")\n",
    "            out[k2] = v\n",
    "        return out\n",
    "\n",
    "    # Case A: explicit checkpoint file path\n",
    "    if ckpt_path is not None:\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint file not found: {ckpt_path}\")\n",
    "\n",
    "        obj = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "        # Packaged format: contains cfg + model_state_dict\n",
    "        if isinstance(obj, dict) and (\"cfg\" in obj) and (\"model_state_dict\" in obj):\n",
    "            cfg = obj[\"cfg\"]\n",
    "            if hasattr(cfg, \"to_container\"):\n",
    "                arch_cfg = dict(cfg.arch)  # type: ignore\n",
    "            else:\n",
    "                arch_cfg = dict(cfg[\"arch\"])  # type: ignore\n",
    "\n",
    "            model_cls = load_model_class(arch_cfg[\"name\"])  # type: ignore\n",
    "            model = model_cls(arch_cfg).to(device)\n",
    "            # Be lenient about prefixes in packaged files\n",
    "            state = obj[\"model_state_dict\"]  # type: ignore\n",
    "            if isinstance(state, dict):\n",
    "                state = _normalize_state_keys(state)\n",
    "            model.load_state_dict(state)  # type: ignore\n",
    "            model.eval()\n",
    "            return model, arch_cfg\n",
    "\n",
    "        # Raw state_dict format: need YAML config nearby (in same dir or checkpoint_dir)\n",
    "        cfg_dir = os.path.dirname(ckpt_path)\n",
    "        cfg_path = os.path.join(cfg_dir, \"all_config.yaml\")\n",
    "        if (checkpoint_dir is not None) and (not os.path.exists(cfg_path)):\n",
    "            cfg_path = os.path.join(checkpoint_dir, \"all_config.yaml\")\n",
    "        if yaml is None:\n",
    "            raise RuntimeError(\"PyYAML is required to load local checkpoints. Please install pyyaml.\")\n",
    "        assert os.path.exists(cfg_path), f\"Missing config: {cfg_path}\"\n",
    "\n",
    "        with open(cfg_path, \"r\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        arch_cfg = cfg[\"arch\"]\n",
    "\n",
    "        model_cfg = dict(\n",
    "            **{k: v for k, v in arch_cfg.items() if k != \"name\"},\n",
    "            batch_size=1,\n",
    "            seq_len=data_meta[\"seq_len\"],\n",
    "            vocab_size=data_meta[\"vocab_size\"],\n",
    "            num_puzzle_identifiers=data_meta[\"num_puzzle_identifiers\"],\n",
    "        )\n",
    "        model_cls = load_model_class(arch_cfg[\"name\"])  # e.g., \"hrm.hrm_act_v1@HierarchicalReasoningModel_ACTV1\"\n",
    "        model = model_cls(model_cfg).to(device)\n",
    "\n",
    "        state = obj if isinstance(obj, dict) else obj.state_dict()\n",
    "        state = _normalize_state_keys(state)\n",
    "        model.load_state_dict(state, assign=True)\n",
    "        model.eval()\n",
    "        return model, arch_cfg\n",
    "\n",
    "    if checkpoint_dir is not None:\n",
    "        # Expect a training directory with all_config.yaml and a step_* file\n",
    "        if yaml is None:\n",
    "            raise RuntimeError(\"PyYAML is required to load local checkpoints. Please install pyyaml.\")\n",
    "\n",
    "        cfg_path = os.path.join(checkpoint_dir, \"all_config.yaml\")\n",
    "        assert os.path.exists(cfg_path), f\"Missing config: {cfg_path}\"\n",
    "\n",
    "        with open(cfg_path, \"r\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "\n",
    "        arch_cfg = cfg[\"arch\"]\n",
    "\n",
    "        # Build model config using dataset-derived fields\n",
    "        model_cfg = dict(\n",
    "            **{k: v for k, v in arch_cfg.items() if k != \"name\"},\n",
    "            batch_size=1,\n",
    "            seq_len=data_meta[\"seq_len\"],\n",
    "            vocab_size=data_meta[\"vocab_size\"],\n",
    "            num_puzzle_identifiers=data_meta[\"num_puzzle_identifiers\"],\n",
    "        )\n",
    "\n",
    "        model_cls = load_model_class(arch_cfg[\"name\"])  # e.g., \"hrm.hrm_act_v1@HierarchicalReasoningModel_ACTV1\"\n",
    "        model = model_cls(model_cfg).to(device)\n",
    "\n",
    "        # Pick the latest step_* file if not specified\n",
    "        step_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"step_\") and \".\" not in f]\n",
    "        assert len(step_files), f\"No step_* files found in {checkpoint_dir}\"\n",
    "        step_files.sort(key=lambda s: int(s.split(\"_\")[1]))\n",
    "        ckpt_path = os.path.join(checkpoint_dir, step_files[-1])\n",
    "\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        state = _normalize_state_keys(state)\n",
    "        model.load_state_dict(state, assign=True)\n",
    "\n",
    "        model.eval()\n",
    "        return model, arch_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2bb5dd3-b1e9-43f7-8744-70f0af6d4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model on cuda\n",
      "HierarchicalReasoningModel_ACTV1(\n",
      "  (inner): HierarchicalReasoningModel_ACTV1_Inner(\n",
      "    (embed_tokens): CastedEmbedding()\n",
      "    (lm_head): CastedLinear()\n",
      "    (q_head): CastedLinear()\n",
      "    (puzzle_emb): CastedSparseEmbedding()\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "    (H_level): HierarchicalReasoningModel_ACTV1ReasoningModule(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x HierarchicalReasoningModel_ACTV1Block(\n",
      "          (self_attn): Attention(\n",
      "            (qkv_proj): CastedLinear()\n",
      "            (o_proj): CastedLinear()\n",
      "          )\n",
      "          (mlp): SwiGLU(\n",
      "            (gate_up_proj): CastedLinear()\n",
      "            (down_proj): CastedLinear()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (L_level): HierarchicalReasoningModel_ACTV1ReasoningModule(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x HierarchicalReasoningModel_ACTV1Block(\n",
      "          (self_attn): Attention(\n",
      "            (qkv_proj): CastedLinear()\n",
      "            (o_proj): CastedLinear()\n",
      "          )\n",
      "          (mlp): SwiGLU(\n",
      "            (gate_up_proj): CastedLinear()\n",
      "            (down_proj): CastedLinear()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "halt_max_steps: 16\n",
      "Selected puzzle index: 233585\n",
      "Input puzzle (0 means blank):\n",
      "0 0 0 | 6 0 1 | 0 0 0\n",
      "0 0 7 | 4 3 0 | 0 8 0\n",
      "0 0 0 | 5 2 0 | 9 0 3\n",
      "- - - - - - - - - - -\n",
      "0 0 0 | 0 0 0 | 3 0 2\n",
      "6 0 5 | 0 0 0 | 0 0 0\n",
      "0 7 0 | 0 4 0 | 0 0 0\n",
      "- - - - - - - - - - -\n",
      "0 8 0 | 0 0 0 | 0 3 5\n",
      "0 0 3 | 0 6 2 | 0 9 0\n",
      "9 1 0 | 0 0 5 | 6 0 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path: str = \"data/sudoku-extreme-1k-aug-1000\"\n",
    "checkpoint_dir: Optional[str] = None\n",
    "ckpt_path: Optional[str] = \"checkpoints/sudoku/checkpoint\"\n",
    "hf_repo: str = \"sapientinc/HRM-checkpoint-sudoku-extreme\"\n",
    "hf_filename: str = \"model.pt\",\n",
    "puzzle_index: Optional[int] = None\n",
    "seed: int = 320\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_path, \"test\", \"dataset.json\")):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {data_path}. Build it via dataset/build_sudoku_dataset.py or adjust data_path.\"\n",
    "    )\n",
    "\n",
    "meta = load_sudoku_metadata(data_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, arch_cfg = load_model_from_checkpoint(\n",
    "    device=device,\n",
    "    data_meta=meta,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    ckpt_path=ckpt_path,\n",
    "    hf_repo=hf_repo,\n",
    "    hf_filename=hf_filename,\n",
    ")\n",
    "\n",
    "print(\"Loaded model on\", device)\n",
    "print(model)\n",
    "print(\"halt_max_steps:\", arch_cfg.get(\"halt_max_steps\"))\n",
    "\n",
    "batch, chosen_index = load_random_sudoku_example(data_path, puzzle_index=puzzle_index)\n",
    "# Move tensors to device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "print(f\"Selected puzzle index: {chosen_index}\")\n",
    "print(\"Input puzzle (0 means blank):\")\n",
    "# Tokens are 1..10 (PAD=0). Subtract 1 for human-readable 0..9.\n",
    "pretty_print_sudoku((batch[\"inputs\"].view(9, 9)))\n",
    "\n",
    "# Step-by-step execution\n",
    "# Ensure carry tensors are allocated on the same device as the model\n",
    "with torch.device(\"cuda\" if device.type == \"cuda\" else \"cpu\"):\n",
    "    carry = model.initial_carry(batch)\n",
    "final_outputs = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b96a44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 81])\n",
      "torch.Size([1, 81])\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"inputs\"].shape)\n",
    "print(batch[\"labels\"].shape)\n",
    "print(batch[\"puzzle_identifiers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48d6fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pad_id': 0, 'ignore_label_id': 0, 'blank_identifier_id': 0, 'vocab_size': 11, 'seq_len': 81, 'num_puzzle_identifiers': 1, 'total_groups': 422786, 'mean_puzzle_examples': 1.0, 'sets': ['all']}\n"
     ]
    }
   ],
   "source": [
    "print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b53e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hillenr/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhillenr\u001b[0m (\u001b[33mhillenr-home\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wb_token=\"d473a6902feca68e46d1adb185325ade4b9bf0d8\"\n",
    "wandb.login(key=wb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18b30691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhillenr\u001b[0m (\u001b[33mhillenr-home\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/hillenr/dev/ml/HRM/wandb/run-20250831_102438-czygs0ed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHierarchicalReasoningModel_ACTV1 judicious-dinosaur\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hillenr-home/Sudoku-extreme-1k-aug-1000%20ACT-torch\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hillenr-home/Sudoku-extreme-1k-aug-1000%20ACT-torch/runs/czygs0ed\u001b[0m\n",
      "[Rank 0, World Size 1]: Epoch 0\n",
      " 50%|████████████████▌                | 10000/20000 [1:56:06<1:33:18,  1.79it/s]W0831 12:20:55.742000 128295 torch/_inductor/utils.py:1137] [0/1] Not enough SMs to use max_autotune_gemm mode\n",
      "[Rank 0, World Size 1]: Epoch 2000\n",
      "100%|███████████████████████████████████| 20000/20000 [5:28:24<00:00,  1.81it/s]\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            num_params ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train/accuracy ▁▁▁▇▁▁▁▁█▇▁▁▁▁▁▁▅▅▁▂▁▅▆▅▆▃▁▃▃▄▃▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/count ▁█▁█████▁▁█▁▁▁██▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁██▁▁▁██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train/exact_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/lm_loss ▄▂▂▂▂▁▁▁▁▁██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/lr ▁▂▂█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_continue_loss ▄▅▂▁▁▃█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_halt_accuracy ▁▁▁▁████▁▁██▁████▁▁▁▁▁▁██▁▁▁███▁▁▁▁▁█▁█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/q_halt_loss ██▇▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/steps ▁▁█▁███▁██▁▁█▁▁▁█▁▁▁▁█▁█▁▁▁█▁▁█▁▁██▁▁█▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            num_params 27275266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train/accuracy 0.15088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/count 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train/exact_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/lm_loss 2.15868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/lr 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_continue_loss 0.03116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/q_halt_accuracy 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/q_halt_loss 0.00049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/steps 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mHierarchicalReasoningModel_ACTV1 judicious-dinosaur\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/hillenr-home/Sudoku-extreme-1k-aug-1000%20ACT-torch/runs/czygs0ed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/hillenr-home/Sudoku-extreme-1k-aug-1000%20ACT-torch\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250831_102438-czygs0ed/logs\u001b[0m\n",
      "100%|███████████████████████████████████| 20000/20000 [7:25:53<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=8 python pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=4000 eval_interval=2000 global_batch_size=200 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7fabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
